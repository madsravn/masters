\chapter{Related Work}
\label{ch:relatedwork}


In this chapter a simplification of the work done by \citet{chanetal} is presented followed by their original data structure. The simplification is the theoretical work of this thesis. Conceptually, the original structure by \citet{chanetal} can be thought of as an extension of the simplified data structure. This way the reader will be introduced to the concepts at an incremental level. The theory behind a $kD$-tree will also be explained as it is the de facto standard of orthogonal range queries today and will be used to compare the practical results of the simplified data structure. \todo{rewrite} Going forward, we will use \emph{ORS} as shorthand for \emph{Original Range Search} and \emph{SRS} as shorthand for \emph{Simplified Range Search}.

The SRS data structure has a time complexity of $\mathcal{O}(\lg n + k\cdot \lg^\epsilon n)$ which is greather than the time complexity of the ORS data structure with $\mathcal{O}(\lg \lg n + (1+k)\cdot \lg^\epsilon n)$. However, the SRS data structure is far more simple - both in code and the auxilary data structures used. Because there is much more internal communication between the data structures in the ORS data structure and much more code to be executed, it is safe to assume that the running time constant of ORS far exceeds that of SRS, making the SRS data structure faster than the ORS data structure in practise. \todo{rephrase}

Some sections will start with a \emph{preliminaries} subsection. This subsection will describe some of the auxiliary data structures used in the section.  It will make the reader acquinted with the data structures when they are referenced.

The query time of the main data structures in this chapter are all \emph{output-sensitive}, meaning that their running time depends on the amount of results found. The data structures themselves are static: After the initial construction of the data structures they will not be altered by insertions or deletions. \todo{more} 

\section{kd-trees}
\label{sect:kdtrees}

The current standard of range reporting is kd-trees. This data structure will be used as a reference point when evaluting the results of the primary work of the thesis. With linear space it is a fitting data structure for range reporting on the RAM, and a practical solution.

A kd-tree is constructed recursively: Given $n$ points, the median of the points with respect to x is found. All points which has an x-coordinate larger than the median goes to the right child, while points which has an x-coordinate smaller than the median, and the median point, goes to the left child. At the next level the points will be divided in a similar fashion, this time using the y-median and the y-coordinates instead. When dividing $n$ points, the median will be chosen as the $\lceil n/2 \rceil$-th smallest number \cite{compgeo}. Therefore a node will contain the line dividing the points given to its left child from the points given to its right child.
Alternating between focussing on the x-coordinates or the y-coordinates at each level, the points are divided until only one point remains in a node. This node will then be a leaf containing that point. Thus, we end up with $n$ leaves. This data structure takes up $\mathcal{O}(n)$ space.

In order to search in this tree, we introduce the term \emph{region}. A region of a node is the area of which its point lie within. The root contains all points and has the biggest region. Since each node contains a line dividing its points between both of its children, we can use this line to narrow the region of both children. Doing this halves the amount of points lying within the region. Now given a search query $q = [x_1, x_2] \times [y_1, y_2]$ and a kd-tree one of three things can happen. The region of a node can be fully contained in the search query, in which case the entire node and the points contained in its subtree are returned as part of the result. The region of a node and the search query can overlap in which case we continue the search down both of the children of the node. Finally the region of a node and the search query has nothing in common in which case the search at that node stops. If a leaf is visited in the search, the point stored in the leaf is reported as part of the result if it lies within the search query. 

Copying a point takes $O(1)$ time. Thus, given a node, the time to report the points stored in the subtree at the node is linear in the number of points reported. It then takes $\mathcal{O}(k_v)$ time to report back all the $k_V$ points which lie in the subtree of a node $v$ which is fully contained in the search region. In order to bound the case where the region of a node is not fully contained in the search region, a vertical line through the region of the root will be used. Conceptually, this vertical line can serve as either the left or right edge of the search region. Consider that the search region does not fully contain regions of any node, this line will describe the amount of regions the search query overlaps. $Q(n)$ will denote the amount of regions the vertical line intersects. In order to find the amount of regions intersected by the line, we need to recall how the kd-tree is built. First a region is split in one dimension and then in the other dimension, resulting in four regions every other step. The vertical line will intersect two of these regions. The vertical line will also intersect the region of the root. When building the kd-tree, the points are split by the x-coordinates. The vertical line will therefore only intersect the region of one of children of the root. Thus, the running time $Q(n)$ of a query to the kd-tree with $n$ points can be described by the recurrence:

\begin{align*}
  Q(n) = \begin{cases}
    \mathcal{O}(1), & \text{if } n = 1,\\
    2 + 2Q(n/4), & \text{if } n > 1
  \end{cases}
\end{align*}

Solving this recurrence gives the solution $Q(n) = \mathcal{O}(\sqrt{n})$. A vertical line through the region of the root in a kd-tree will intersect $\sqrt{n}$ regions. 

%% We can also imagine the regions of the kd-tree to form a $m \times m$ grid in the region of the root. Each region in the grid contains one point, and a vertical line through the grid will intersect $m$ regions - one at each row. Since each region in the grid contains a point, we have $m \times m = n$ which gives $m = \sqrt{n}$. Thus, a vertical line through the region of the root will intersect $\sqrt{n}$ regions.

Searching the kd-tree takes $\mathcal{O}(\sqrt{n} + k)$ time to report $k$ points as a result. When the amount of points reported back as result of the search query is low, the query time per point is relatively high. Another thing to notice is that there is no time penalty per point reported. Just searching through the data structure costs $\mathcal{O}(\sqrt{n})$ time, but the time to report back points are linear to the amount of points.


The tree with $n$ points can be represented as flat array with $n$ entries. The $\lceil n/2 \rceil$-th element in the array is the root of the tree. \todo{uddyb}

\section{Range trees}
\label{sect:rangetrees}

The range tree is a data structure which supports range queries. The space complexity of this data structure is $\mathcal{O}(n\lg n)$. A normal query in a range tree uses $\mathcal{O}(\lg^2 n + k)$ time to report $k$ points. This time can be optimized to $\mathcal{O}(\lg n + k)$ without changing the space complexity using \emph{fractional cascading}. We will first look at how the data structure is built and how it is used for range reporting. Then we will introduce fractional cascading and see how that will change the query time. With a space complexity of $\mathcal{O}(n \lg n)$ words this data structure is not going to replace the kd-tree. Instead the range tree will serve as a way to introduce some of the ideas behind the SRS and ORS data structures. 

Consider a balanced binary search tree with $n$ keys for a $1$-dimensional query. In order to answer the query $q = [x_1, x_2]$ the following is done: From the root, travel to the \emph{least common ancestor} of $x_1$ and $x_2$. This is the node whose subtree contains both $x_1$ and $x_2$, but $x_1$ lies in the left subtree, while $x_2$ lies in the right subtree. From the least common ancestor, travel to both $x_1$ and $x_2$. While traveling to $x_1$, the first step is the left child of $x_1$. From here, everytime a left child is chosen as the next step in the path, the subtree in the right child will only contain points between $x_1$ and $x_2$. This entire subtree is reported back as results. Symmetrically, the same is done with the path to $x_2$. When a right child is chosen as the next step, the subtree in the left child is reported back as results. In a $1$-dimensional search, when a node has a subtree which only contains points in the search range, the node is said to be \emph{fully contained}.

A balanced binary search tree has a space complexity of $\mathcal{O}(n)$. Reporting back the points stored in a subtree requires time linear to the amount of points in the subtree. Travelling from the root to $x_1$ and $x_2$ requires $\mathcal{O}(\lg n)$ time. Hence, the query time of a $1$-dimensional search query is $\mathcal{O}(\lg n + k)$.

Range reporting in a $2$-dimensional space on the kd-tree is done by using $1$-dimensional sub-queries. The kd-tree alternates in which dimension the search is performed. The range tree also searches by using $1$-dimensional sub-queries, but instead of alternating between dimensions, it separates them. Given a search query $q = [x_1, x_2] \times [y_1, y_2]$, it will first find the points lying in the range of $[x_1, x_2]$. Among those points, it will find the points lying in the range of $[y_1, y_2]$. This leaves us with all the points lying in the search query.

Doing the first $1$-dimensional search is exactly what is accomplished using a balanced binary search tree. A balanced binary search tree is built to support range search on the x-axis of all of the points. We will call this tree the primary tree. Then for each internal node in the primary tree a new balanced binary search tree is built to support range search on the y-axis of the points in the subtree of that node. We call these balanced binary search trees for auxiliary trees. The primary tree holds pointers to the auxillary tree for each node.

A range query $q = [x_1, x_2] \times [y_1, y_2]$ on the range tree is answered in the following way. From the least common ancestor of $x_1$ and $x_2$, the search travels down to $x_1$ and $x_2$. On the way to $x_1$ and $x_2$, each node that is fully contained in $[x_1, x_2]$ will be flagged. Using the auxilliry tree of each node that is flagged, a search will be done to find the points in the range $[y_1, y_2]$.

The height of a balanced binary search tree containing $n$ points is $\lg n$. Each point $p$ in the primary tree is only stored in the auxillary trees of nodes on the path to the leaf containing the point $p$. This means that each point $p$ is only stored once per level in the primary tree. Each auxillary tree uses space linear to the amount of points it holds. Thus, the space complexity of a range tree is bounded by $\mathcal{O}(n \lg n)$.

The query time for each auxillary tree that is searched is $\mathcal{O}(\lg n + k_v)$, where $k_v$ is the amount of points that is reported back by the auxillary tree at the node $v$ in the primary tree. The amount of auxillary trees which will be searched is bounded by the length of the path from the least common ancestor of $x_1$ and $x_2$ to the leaves containing $x_1$ and $x_2$. This path can at most visit two nodes per level of the primary tree, and the length is thus bounded by $\mathcal{O}(\lg n)$. The query time of a range search in the range tree is then 

\begin{align*}
  \sum\limits_{v} \mathcal{O}(\lg n + k_v) = \mathcal{O}(\lg^2 n + k)
\end{align*}

Fractional cascasding can be used to speed up the query time without changing the space complexity of the data structure. Instead of using a balanced binary search tree as the auxillary data structure, we are going to use an array. This array will contain the same points as the auxillary balanced binary search tree did. The points in the array will be sorted by their y-coordinate. At the node $v$, each entry in the array will contain a point and two pointers. One pointer will be pointing to an entry in the auxillary array of the left child of $v$, while the other pointer will be pointing to an entry in the auxillary array of the rigth child of $v$. We call these the left pointer and the right pointer, respectively. Suppose that $A_v[i]$ stores a point $p$. Then the left pointer from $A_v[i]$ will be pointing to the first entry in the left childs auxillary array containing a point with a y-coordinate greater or equal to $p_y$. The same applies to the right pointer of $A_v[i]$, pointing to the right child instead of the left child. \\

Searching the range tree with fractional cascasding starts by finding the least common ancestor of $x_1$ and $x_2$. At this node, a binary search is done in order to find the first entry in the auxillary array which y-coordinate is greater or equal to $y_1$. At any given node, we call the position of this entry $\tau$. We walk from the least common ancestor of $x_1$ and $x_2$ to both $x_1$ and $x_2$, finding all the nodes which are fully contained in $[x_1, x_2]$. Each time a left child is chosen on the path to $x_1$ or $x_2$, the left pointer is used to update the position at $\tau$. When a right child is chosen on the path, the position of $\tau$ is updated using the right pointer. When a fully contained node is found, we look in the auxillary array from the position of $\tau$ and $k_v$ entries forward in order to report $k_v$ points back as result. This is done by incrementing the position of $\tau$ until the point at that entry no longer is within the range of $[y_1, y_2]$. This takes $\mathcal{O}(1+k_v)$ time. The total query time now becomes
% Each time a left child is chosen on the path to $x_1$ or $x_2$, the left pointer is used to update which entry in the auxillary array has the first point with a y-coordinate greater or equal to $y_1$. 

\begin{align*}
  \sum\limits_{v} \mathcal{O}(1 + k_v) = \mathcal{O}(\lg n + k)
\end{align*}



